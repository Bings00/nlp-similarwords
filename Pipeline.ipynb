{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting the emergence of new terminology and establishing automated ways of understanding the meaning of these terms (by analyzing their similar words)\n",
    "\n",
    "<br /> \n",
    "<ul>\n",
    "<li>Analyzing the feasibility of maintaining word lists that are automatically augmented with similar words from word embedding models.</li>\n",
    "<li>Because words derive their meanings from the context words that they keep!</li>\n",
    "</ul>\n",
    "<br /> \n",
    "\n",
    "#### General Pipeline v.02 (2020 July)\n",
    "1. Scan the entire data file and count the number of unique words (create a dictionary at this time)\n",
    "<ul>\n",
    "    <li>Set a minimum frequency (i.e. 5) for dropping less frequent words</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import io \n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "from pdb import set_trace as bp\n",
    "from string import punctuation\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "\n",
    "import gensim.models\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename of the original corpus\n",
    "fname = \"wiki_processed.txt\"\n",
    "\n",
    "if not os.path.isfile(fname):\n",
    "    # double check if the data file exists\n",
    "    print(\"File {} does not exist. Exiting...\".format(fname))\n",
    "    #sys.exit()\n",
    "    \n",
    "# A lookup table {key: None for key in string.punctuation}\n",
    "punc_table = str.maketrans(dict.fromkeys(punctuation))  \n",
    "vocab_dict = {} # A dictionary comprehension\n",
    "\n",
    "with open(fname, encoding='utf8', errors='ignore') as fp:\n",
    "    cnt = 0\n",
    "    for line in fp:\n",
    "        #print(\"processing line {} \".format(cnt))\n",
    "        line = line.lower().strip()\n",
    "        new_line = line.translate(punc_table)  # Output: string without punctuation\n",
    "\n",
    "        for word in new_line.split():\n",
    "            # only add word to the dictionary when the key does not exist\n",
    "            if word not in vocab_dict:\n",
    "                vocab_dict[word] = 0\n",
    "            vocab_dict[word] += 1 # word count\n",
    "        #bp()\n",
    "        cnt += 1\n",
    "\n",
    "print(\"Finished reading the data file!\")\n",
    "print(\"Number of unique words in vocab_dict: {}\".format(len(vocab_dict.items())))\n",
    "\n",
    "# Remove less frequent words from the vocab dict\n",
    "unknown_dict = []\n",
    "for word, count in list(vocab_dict.items()):\n",
    "    if count < 5:\n",
    "        unknown_dict.append(word) # save them in an array for the future use\n",
    "        del vocab_dict[word]\n",
    "\n",
    "print(\"After removing less frequent words, \")\n",
    "print(\"Number of unique words in vocab_dict: {}\".format(len(vocab_dict.items()))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Build 3 word embedding models (skip-gram, cbow, glove) and query top K neighbour words for every word in the dictionary (for each word embedding model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"  If True (sg=1), skip-gram is used; if False (sg=0), CBOW is used.  \"\"\"\n",
    "### Building a Skip-gram model with default settings\n",
    "def build_skipgram_model(sentences, model_name):\n",
    "    # sg: 1 for skip-gram; otherwise CBOW\n",
    "    skipgram_model = gensim.models.Word2Vec(sentences=sentences, window=5, min_count=5, seed=1, workers=1, sg=1)\n",
    "    skipgram_model.wv.save_word2vec_format(model_name) # save the model\n",
    "\n",
    "def build_cbow_model(sentences, model_name):\n",
    "    # sg: 1 for skip-gram; otherwise CBOW\n",
    "    cbow_model = gensim.models.Word2Vec(sentences=sentences, window=5, min_count=5, seed=1, workers=1, sg=0)\n",
    "    cbow_model.wv.save_word2vec_format(model_name) # save the model\n",
    "\n",
    "class MyCorpus(object):\n",
    "    \"\"\"An interator that yields sentences (lists of str).\"\"\"\n",
    "    def __iter__(self):\n",
    "        for line in open(fname, encoding='utf8', errors='ignore'):\n",
    "            yield utils.simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = MyCorpus()\n",
    "\n",
    "### This can be a one-time proces\n",
    "print(\"... Start building a Skipgram model ...\")\n",
    "#build_skipgram_model(sentences, 'w2v_model_skipgram')\n",
    "print(\"... Start building a CBOW model ...\")\n",
    "#build_cbow_model(sentences, 'w2v_model_cbow')\n",
    "\n",
    "### Load the built models to save some time\n",
    "#skipgram_model = KeyedVectors.load_word2vec_format(\"w2v_model_skipgram\") \n",
    "#cbow_model = KeyedVectors.load_word2vec_format(\"w2v_model_cbow\") \n",
    "# glove vector file still needs to be generated on Linux environment\n",
    "#glove_model = KeyedVectors.load_word2vec_format(\"wiki_glove.word2vec\") \n",
    "print(\"---> Three word embedding models were loaded!!\")\n",
    "\n",
    "# Get top K similar words for each word in the dictionary\n",
    "with open('Data/original_top10.tsv', 'w', newline='') as fp:\n",
    "    headers = ['target_word', 'top_1', 'top_2', 'top_3', 'top_4', 'top_5', 'top_6', 'top_7', 'top_8', 'top_9', 'top_10']\n",
    "    tsv_writer = csv.writer(fp, delimiter='\\t')\n",
    "    tsv_writer.writerow(headers)\n",
    "    \n",
    "    for word, count in list(vocab_dict.items()):\n",
    "        print(\"Word: {}, Frequency: {}\".format(word, count))\n",
    "        try:\n",
    "            line = []\n",
    "            line.append(word)\n",
    "            for item in cbow_model.similar_by_word(word):\n",
    "                line.append(item) # adding tuples\n",
    "            tsv_writer.writerow(line)\n",
    "        except:\n",
    "            print(\"--> {} not found in model vocabulary\".format(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Pick two random words (i_word and j_word) from the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Pick two random words from the dictionary\n",
    "rand_i = np.random.randint(len(vocab_dict)) \n",
    "rand_j = np.random.randint(len(vocab_dict))\n",
    "assert rand_i != rand_j\n",
    "# list(vocab_dict.items())[rand_i] will return a (word, count) tuple\n",
    "i_word = list(vocab_dict.items())[rand_i][0]\n",
    "j_word = list(vocab_dict.items())[rand_j][0]\n",
    "\n",
    "print(\"Two randomly picked words from the dictionary:\")\n",
    "print(list(vocab_dict.items())[rand_i])\n",
    "print(list(vocab_dict.items())[rand_j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.  Generate new corpus files with random word replacements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_words(line, phrase, window_size):\n",
    "    line = line.split()\n",
    "    phrase = phrase.split()\n",
    "    word_cnt = len(phrase)\n",
    "\n",
    "    for i,word in enumerate(line):\n",
    "        if word == phrase[0] and line[i:i+word_cnt] == phrase:\n",
    "            start = max(0, i-window_size)\n",
    "            left = ' '.join(line[start:i])\n",
    "            right = ' '.join(line[i+word_cnt:i+word_cnt+window_size])\n",
    "            return left + ' ' + right\n",
    "\n",
    "# new data file with random words replacement\n",
    "replaced_fname = \"Data/replaced_\" + i_word +\"_with_\" + j_word + \".txt\"\n",
    "\n",
    "l_i_word = [] # list of sentences that contain target word\n",
    "l_j_word = [] # list of sentences that contain replacing word\n",
    "\n",
    "with open(fname, encoding='utf8', errors='ignore') as fp, open(replaced_fname, 'w', encoding='utf8') as rfp:# replace 100%\n",
    "    cnt = 0\n",
    "    for line in fp:\n",
    "        #print(\"processing line {} \".format(cnt))\n",
    "        line = line.lower().strip()\n",
    "        new_line = line.translate(punc_table)  # Output: string without punctuation\n",
    "        if i_word in new_line.split():\n",
    "            l_i_word.append(get_context_words(new_line, i_word, 5))\n",
    "            # perform word replacement here\n",
    "            line = new_line.replace(i_word,j_word) # re-use the variable?\n",
    "        if j_word in new_line.split():\n",
    "            l_j_word.append(get_context_words(new_line, j_word, 5))\n",
    "        rfp.write(line) # for writing to our new corpus file\n",
    "        rfp.write(\"\\n\")\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Calculate the average Jaccard Similarity (with window size 5) of two random words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "js_sum = 0\n",
    "for i, win_i in enumerate(l_i_word):\n",
    "    for j, win_j in enumerate(l_j_word):\n",
    "        print(\"Processing {} - {} th item ...\".format(i+1,j+1))\n",
    "        # tokenization\n",
    "        tok_i = win_i.split()\n",
    "        tok_j = win_j.split()\n",
    "        # union and intersection of two sets\n",
    "        union = set(tok_i).union(set(tok_j))\n",
    "        intersection = set(tok_i).intersection(set(tok_j))\n",
    "        js_score = len(intersection)/len(union)\n",
    "        print(\"Jaccard similarity score: {}\".format(js_score))\n",
    "        js_sum += js_score\n",
    "\n",
    "\n",
    "## ZeroDivisionError: division by zero <--- needs to be considered\n",
    "avg_js = js_sum / (len(l_i_word)*len(l_j_word))\n",
    "print(\"i_word: {} and j_word: {}\".format(i_word, j_word))\n",
    "print(\"Average Jaccard similarity: {}\".format(avg_js))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Build 3 word embedding models with the new data files and query top K neighbour words for every word in a dictionary again\n",
    "<br>\n",
    "<i>(i_word doesnâ€™t exist in the new data file at this point)</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCorpus_v2(object):\n",
    "    \"\"\"An interator that yields sentences (lists of str).\"\"\"\n",
    "    def __iter__(self):\n",
    "        for line in open(replaced_fname, encoding='utf8', errors='ignore'):\n",
    "            yield utils.simple_preprocess(line)\n",
    "\n",
    "#sentences_v2 = MyCorpus_v2()\n",
    "##### Sanity checking\n",
    "sentences_v2 = MyCorpus()\n",
    "\n",
    "# because we're replacing different random word every time, can't skip building models this time\n",
    "print(\"... Start building a Skipgram model ...\")\n",
    "#build_skipgram_model(sentences_v2, 'w2v_model_skipgram_replaced')\n",
    "print(\"... Start building a CBOW model ...\")\n",
    "#build_cbow_model(sentences_v2, 'w2v_model_cbow_replaced')\n",
    "\n",
    "\n",
    "### Load the built models to save some time\n",
    "#skipgram_model_v2 = KeyedVectors.load_word2vec_format(\"w2v_model_skipgram_replaced\") \n",
    "# cbow and glove?\n",
    "#cbow_model_v2 = KeyedVectors.load_word2vec_format(\"w2v_model_cbow_replaced\") \n",
    "print(\"---> new word embedding models (after random word replacements) were loaded!!\")\n",
    "\n",
    "# Get top K similar words for each word in the dictionary\n",
    "with open('Data/replaced_top10.tsv', 'w', newline='') as fp:\n",
    "    headers = ['target_word', 'top_1', 'top_2', 'top_3', 'top_4', 'top_5', 'top_6', 'top_7', 'top_8', 'top_9', 'top_10']\n",
    "    tsv_writer = csv.writer(fp, delimiter='\\t')\n",
    "    tsv_writer.writerow(headers)\n",
    "    \n",
    "    for word, count in list(vocab_dict.items()):\n",
    "        print(\"Word: {}, Frequency: {}\".format(word, count))\n",
    "        try:\n",
    "            line = []\n",
    "            line.append(word)\n",
    "            for item in cbow_model_v2.similar_by_word(word):\n",
    "                line.append(item) # adding tuples\n",
    "            tsv_writer.writerow(line)\n",
    "        except:\n",
    "            print(\"--> {} not found in model vocabulary\".format(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Analyze those generated CSV/TSV files and detect the replaced words with least overlaps in top K similar word (most likely they have changes in their meaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# original_top10.tsv\n",
    "# replaced_top10.tsv\n",
    "df1 = pd.read_csv(\"Data/original_top10.tsv\", sep='\\t', encoding=\"ISO-8859-1\")\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"Data/replaced_top10.tsv\", sep='\\t', encoding=\"ISO-8859-1\")\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the dataframes -- we only need words from the tuple strings\n",
    "df1['top_1'] = df1['top_1'].str.strip(\"()\").str.replace(\"'\",\"\").str.split(',',expand=True)\n",
    "df1['top_2'] = df1['top_2'].str.strip(\"()\").str.replace(\"'\",\"\").str.split(',',expand=True)\n",
    "df1['top_3'] = df1['top_3'].str.strip(\"()\").str.replace(\"'\",\"\").str.split(',',expand=True)\n",
    "df1['top_4'] = df1['top_4'].str.strip(\"()\").str.replace(\"'\",\"\").str.split(',',expand=True)\n",
    "df1['top_5'] = df1['top_5'].str.strip(\"()\").str.replace(\"'\",\"\").str.split(',',expand=True)\n",
    "df1['top_6'] = df1['top_6'].str.strip(\"()\").str.replace(\"'\",\"\").str.split(',',expand=True)\n",
    "df1['top_7'] = df1['top_7'].str.strip(\"()\").str.replace(\"'\",\"\").str.split(',',expand=True)\n",
    "df1['top_8'] = df1['top_8'].str.strip(\"()\").str.replace(\"'\",\"\").str.split(',',expand=True)\n",
    "df1['top_9'] = df1['top_9'].str.strip(\"()\").str.replace(\"'\",\"\").str.split(',',expand=True)\n",
    "df1['top_10'] = df1['top_10'].str.strip(\"()\").str.replace(\"'\",\"\").str.split(',',expand=True)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_dict = df1.set_index('target_word').T.to_dict('list')\n",
    "print(len(df1_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the dataframes -- we only need words from the tuple strings\n",
    "df2['top_1'] = df2['top_1'].str.strip(\"()\").str.replace(\"'\",\"\").str.split(',',expand=True)\n",
    "df2['top_2'] = df2['top_2'].str.strip(\"()\").str.replace(\"'\",\"\").str.split(',',expand=True)\n",
    "df2['top_3'] = df2['top_3'].str.strip(\"()\").str.replace(\"'\",\"\").str.split(',',expand=True)\n",
    "df2['top_4'] = df2['top_4'].str.strip(\"()\").str.replace(\"'\",\"\").str.split(',',expand=True)\n",
    "df2['top_5'] = df2['top_5'].str.strip(\"()\").str.replace(\"'\",\"\").str.split(',',expand=True)\n",
    "df2['top_6'] = df2['top_6'].str.strip(\"()\").str.replace(\"'\",\"\").str.split(',',expand=True)\n",
    "df2['top_7'] = df2['top_7'].str.strip(\"()\").str.replace(\"'\",\"\").str.split(',',expand=True)\n",
    "df2['top_8'] = df2['top_8'].str.strip(\"()\").str.replace(\"'\",\"\").str.split(',',expand=True)\n",
    "df2['top_9'] = df2['top_9'].str.strip(\"()\").str.replace(\"'\",\"\").str.split(',',expand=True)\n",
    "df2['top_10'] = df2['top_10'].str.strip(\"()\").str.replace(\"'\",\"\").str.split(',',expand=True)\n",
    "df2.head()\n",
    "# dataframe to dictionary\n",
    "df2_dict = df2.set_index('target_word').T.to_dict('list')\n",
    "print(len(df2_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_dict = {} # if target_word exists, calculate the intersection\n",
    "notFound = [] # if target_word is not found\n",
    "for key in df2_dict.keys():\n",
    "    if key in df1_dict.keys():\n",
    "        overlap = set(df2_dict[key]).intersection(df1_dict[key])\n",
    "        overlap_dict[key] = len(overlap)\n",
    "        # debugging purpose\n",
    "        if overlap:\n",
    "            print(\"{} -- {}\".format(key, overlap))  # or save this info into another dictionary\n",
    "    else:\n",
    "        notFound.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(notFound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort dictionary by values(int)\n",
    "from collections import OrderedDict\n",
    "od = OrderedDict(sorted(overlap_dict.items(), key=lambda x: x[1]))\n",
    "od"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = {}\n",
    "zero = []\n",
    "\n",
    "for word, count in list(od.items()):\n",
    "#     if count not in hist:\n",
    "#         hist[count] = 0\n",
    "    hist[count] += 1\n",
    "    if count == 0:\n",
    "        zero.append(word)\n",
    "        \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(hist.keys(), hist.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df2_dict))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
